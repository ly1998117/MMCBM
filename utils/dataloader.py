# -*- encoding: utf-8 -*-"""@Author :   liuyang@github :   https://github.com/ly1998117/MMCBM@Contact :  liu.yang.mine@gmail.com"""import randomimport pandas as pdfrom monai.transforms import apply_transform, Composefrom tqdm import tqdmfrom params import pathology_labels, modality_data_keys, data_info, img_size, modalitiesfrom copy import deepcopyfrom torch.utils.data import DataLoader, Dataset as _Datasetfrom .data_split import DataSplitfrom .my_transformer import *from .logger import PrintColortorch.multiprocessing.set_sharing_strategy('file_system')def train_transforms(normalize=True, img_size=img_size):    print('normal')    return Compose(        [            LoadImage(),            ChannelFirstd(),            Resized(img_size=img_size),            RandFlipd(prob=.5),            RandRotated(range=.3, prob=.5),            RandomBrightnessContrastd(.2),            RandGaussianSmoothd(prob=.2),            RandGaussianNoised(prob=.2),            RandZoomd(min_zoom=0.8, max_zoom=1.2, prob=.5),            ##############################################            # RandElasticd(prob=.2),            # RandShiftIntensityd(prob=.2),            ##############################################            ToTensord(normalize=normalize),        ]    )def val_transforms(normalize=True, img_size=img_size):    return Compose(        [            LoadImage(),            ChannelFirstd(),            Resized(img_size=img_size),            ToTensord(normalize=normalize),        ]    )def test_transforms(normalize=True, img_size=img_size):    return Compose(        [            LoadImage(),            ChannelFirstd(),            Resized(img_size=img_size),            ToTensord(normalize=normalize),        ]    )def clip_transforms(trans):    return ClipTrans(trans)class ImagesReader:    def __init__(self, transform=val_transforms(normalize=True)):        self.transform = transform    def __call__(self, data_i: dict, pathology=None, name=None):        """        Fetch single data item.        data_i: {'FA': [path1, path2, path3], 'ICGA': [path1, path2, path3], 'US': [path1, path2, path3],        'label': 0, 'pathology': 0, 'name': 0}        """        data_i = {m: [i for i in v if i is not None] for m, v in data_i.items() if len(v) > 0}        data_i = {m: v for m, v in data_i.items() if len(v) > 0}        data_i = dict(data=data_i, pathology=[pathology], name=[name], img={m: v for m, v in data_i.items()},                      modality=['MM'] if len(data_i) > 1 and 'US' in list(data_i.keys()) else list(data_i.keys()),                      label=[None])        if self.transform is not None:            data_i['data']: dict = apply_transform(self.transform, data_i['data'])        data_i['data'] = {m: torch.stack(d, dim=0).unsqueeze(0) if isinstance(d, list) else d.unsqueeze(0)                          for m, d in data_i['data'].items()}        if pathology is not None:            data_i['label'] = torch.tensor(pathology_labels[pathology], dtype=torch.long)        return data_iclass BatchDictDataset(_Dataset):    """        get rows from batched data    """    def __init__(self, cache_dir, cache_name):        self.keys = []        self.values = []        os.makedirs(cache_dir, exist_ok=True)        self.path: str = os.path.join(cache_dir, cache_name)        self.cache_finished = False        self.load()    def __len__(self):        return len(self.values[0])    def __getitem__(self, idx):        return {k: self.slice(v, idx) for k, v in zip(self.keys, self.values)}    def slice(self, data, idx):        if isinstance(data, torch.Tensor):            return data[idx]        elif isinstance(data, np.ndarray):            return data[idx]        elif isinstance(data, (str, int, float, bool)):            return data        elif isinstance(data, (list, tuple)):            if isinstance(data[0], (list, tuple, dict, np.ndarray, torch.Tensor)):                return [self.slice(d, idx) for d in data]            return data[idx]        elif isinstance(data, dict):            return {k: self.slice(v, idx) for k, v in data.items()}        else:            raise ValueError(f'Unsupported type: {type(data)}')    def concat(self, old_v, value):        if isinstance(value, torch.Tensor):            old_v = torch.tensor(old_v).type(value.dtype)            old_v = torch.cat([old_v, value], dim=0)        elif isinstance(value, np.ndarray):            old_v = np.array(old_v).astype(value.dtype)            old_v = np.concatenate([old_v, value], axis=0)        elif isinstance(value, (str, int, float, bool)):            if not isinstance(old_v, list):                old_v = [old_v]            old_v.append(value)        elif isinstance(value, (list, tuple)):            if isinstance(value[0], (list, tuple, dict, np.ndarray, torch.Tensor)):                if len(old_v) == 0:                    old_v = value                else:                    for idx in range(len(value)):                        old_v[idx] = self.concat(old_v[idx], value[idx])            else:                old_v = list(old_v)                old_v.extend(value)        elif isinstance(value, dict):            old_v = dict(old_v)            for k in value.keys():                if k not in old_v:                    old_v[k] = []                old_v[k] = self.concat(old_v[k], value[k])        else:            raise ValueError(f'Unsupported type: {type(value)}')        return old_v    def __setitem__(self, key, value):        if self.cache_finished:            PrintColor.print(f'Cache dataset is finished, cannot add more data', color='red')            return        if key not in self.keys:            self.keys.append(key)            self.values.append([])        idx = self.keys.index(key)        self.values[idx] = self.concat(self.values[idx], value)    def add(self, dict_data):        for k, v in dict_data.items():            self.__setitem__(k, v)    def save(self):        if self.cache_finished:            return        PrintColor.print(f'Cache dataset to {self.path}', color='green')        data = {k: v for k, v in zip(self.keys, self.values)}        torch.save(data, self.path)    def load(self):        if os.path.exists(self.path):            PrintColor.print(f'Load dataset from {self.path}', color='green')            data = torch.load(self.path)            for k, v in data.items():                self.__setitem__(k, v)            self.cache_finished = Trueclass Dataset(_Dataset):    def __init__(self, data: pd.DataFrame, transform, prob=.1,                 noise=False, time_shuffle=False, n_shot='full'):        super(Dataset, self).__init__()        self.transform = transform        self.R = np.random.RandomState(42)        self.prob = prob        self.n_shot = n_shot        self.data = self.prepare_data(data)        self.noise = noise        self.time_shuffle = time_shuffle        self.patient = None        print(f'modality: {self.data[0]["modality"]}  noise: {noise}')    def prepare_data(self, data):        df = data.reset_index(drop=True)        if self.n_shot != 'full':            df = df.sample(n=self.n_shot, random_state=42)        self.labels = pathology_labels        df['label'] = df['pathology'].map(lambda data_i: pathology_labels[data_i])        return df.to_dict('records')    def do(self):        return self.R.rand() < self.prob    def set_patient(self, patient):        self.patient = patient    def unset_patient(self):        self.patient = None    def __len__(self):        return len(self.data)    def __getitem__(self, index: int):        """        Fetch single data item from `self.data`.        """        if self.patient is not None:            data_i = [d for d in self.data if d['name'] == self.patient]            if len(data_i) == 0:                raise KeyError(f'patient {self.patient} not found')            data_i = data_i[0]        else:            data_i = self.data[index]        data_i['img'] = {m: list(map(lambda x: x.split('/')[-1], pl)) for m, pl in data_i['path'].items()}        data_i['data'] = deepcopy(data_i['path'])        if self.transform is not None:            data_i['data']: dict = apply_transform(self.transform, data_i['data'])        data_i['data'] = {m: torch.stack(d, dim=0) if isinstance(d, list) else d                          for m, d in data_i['data'].items()}        if self.do() and self.noise:            data_i['label'] = torch.tensor(self.labels['noise'], dtype=torch.long)            data_i['data'] = {m: torch.randn_like(d) for m, d in data_i['data'].items()}        if self.do() and self.time_shuffle:            data_i['data'] = {m: d[:, self.R.permutation(d.shape[1]), ...] for m, d in data_i['data'].items()}        return {'name': data_i['name'], 'pathology': data_i['pathology'], 'modality': data_i['modality'],                'img': data_i['img'], 'data': data_i['data'], 'label': data_i['label']}class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler[int]):    """Samples elements randomly from a given list of indices for imbalanced dataset    Arguments:        indices (list, optional): a list of indices        num_samples (int, optional): number of samples to draw    """    def __init__(self, dataset, indices=None):        # if indices is not provided,        # all elements in the dataset will be considered        super().__init__(dataset)        self.dataset = dataset        self.indices = list(range(len(dataset))) \            if indices is None else indices        # if num_samples is not provided,        # draw `len(indices)` samples in each iteration        self.num_samples = len(self.indices)        # distribution of classes in the dataset        label_to_count = {}        for idx in self.indices:            label = self._get_label(idx)            if label not in label_to_count:                label_to_count.update({label: 0})            label_to_count[label] += 1        # weight for each sample        weights = [1.0 / label_to_count[self._get_label(idx)]                   for idx in self.indices]        self.weights = torch.DoubleTensor(weights)    def _get_label(self, idx):  # Note: for single attribute dataset        return self.dataset.data[idx]['label']  # [0]    def __iter__(self):        idx = (self.indices[i] for i in torch.multinomial(            self.weights, self.num_samples, replacement=True))        return idx    def __len__(self):        return self.num_samplesdef get_dataloader(batch_size, num_worker, files, trans, shuffle, imbalance,                   noise=False, time_shuffle=False, n_shot='full'):    dataset = Dataset(data=files,                      transform=trans,                      noise=noise,                      time_shuffle=time_shuffle,                      n_shot=n_shot                      )    dataloader = DataLoader(        sampler=ImbalancedDatasetSampler(dataset),        dataset=dataset,        batch_size=batch_size,        shuffle=False,        num_workers=num_worker,        pin_memory=False,        persistent_workers=False    ) if imbalance else DataLoader(        dataset=dataset,        batch_size=batch_size,        shuffle=shuffle,        num_workers=num_worker,        pin_memory=False,        persistent_workers=False    )    return dataloaderclass MMDataLoader:    def __init__(self, reader, mode, modality, k_fold, batch_size, train_trans, val_trans,                 num_worker=0, imbalance=False,                 noise=False, time_shuffle=False,                 modality_shuffle=False, n_shot='full'):        self.loaders = []        self.modality = modality        self.batch_size = batch_size        self.modality_shuffle = modality_shuffle        self.stage_name = mode        self.num_workers = num_worker        if mode == 'test':            files = reader.get_test_data()            self.shuffle = False            if files is not None and not files.empty:                self.loaders.append(                    get_dataloader(batch_size=batch_size,                                   num_worker=num_worker,                                   files=files,                                   trans=val_trans,                                   shuffle=False,                                   imbalance=imbalance,                                   noise=noise,                                   time_shuffle=time_shuffle,                                   )                )        else:            for m in modality_data_keys[modality]:                train_files, val_files = reader.get_data_split(m, k_fold)                if mode == 'train':                    files = train_files                    trans = train_trans                    self.shuffle = True                elif mode == 'valid':                    files = val_files                    trans = val_trans                    self.shuffle = False                else:                    raise KeyError(f'invalid mode: {mode}')                if files is not None and not files.empty:                    self.loaders.append(                        get_dataloader(batch_size=batch_size,                                       num_worker=num_worker,                                       files=files,                                       trans=trans,                                       shuffle=self.shuffle,                                       imbalance=imbalance,                                       noise=noise,                                       time_shuffle=time_shuffle,                                       n_shot=n_shot if mode == 'train' else 'full'                                       )                    )        self.loader = None    def to_cpu(self, x):        return self._to(x, 'cpu')    def to_cuda(self, x, device):        return self._to(x, device)    def _to(self, x, device):        if isinstance(x, torch.Tensor):            x = x.to(device)        if isinstance(x, (list, tuple)):            x = [self._to(_x, device) for _x in x]        if isinstance(x, dict):            x = {k: self._to(v, device) for k, v in x.items()}        return x    def __len__(self):        return sum(map(lambda x: len(x), self.loaders))    def empty(self):        return len(self.loaders) == 0    def get_loader(self):        if len(self.iter_loaders) == 0:            raise StopIteration        if self.shuffle:            return random.choice(self.iter_loaders)        if self.loader is None:            for loader in self.iter_loaders:                return loader        if self.loader in self.iter_loaders:            self.idx = self.iter_loaders.index(self.loader)        return self.iter_loaders[(self.idx + 1) % len(self.iter_loaders)]    def remove_loader(self, loader):        self.iter_loaders.remove(loader)        if len(self.iter_loaders) == 0:            raise StopIteration    def get_patient(self, name):        for loader in self.loaders:            loader.dataset.set_patient(name)            for batch in loader:                loader.dataset.unset_patient()                return batch        return None    @torch.no_grad()    def cache_data(self, backbone, device='cpu', dir_path='.'):        cache = {}        start_str = f'{self.stage_name}_{self.modality}_'        if (os.path.exists(dir_path) and len(os.listdir(dir_path)) > 0 and                any([f.startswith(start_str) for f in os.listdir(dir_path)])):            for f in os.listdir(dir_path):                if f.startswith(f'{self.stage_name}_{self.modality}_'):                    cache[f.split('_')[-1].split('.')[0]] = BatchDictDataset(cache_dir=dir_path, cache_name=f)        else:            for data in tqdm(self, desc=f'Caching data [MMDataLoader {self.stage_name} {self.modality}]'):                modality = data['modality'][0]                if modality not in cache:                    cache[modality] = BatchDictDataset(cache_dir=dir_path,                                                       cache_name=f'{self.stage_name}_{self.modality}_{modality}.pth')                inp = self.to_cuda(data['data'], device=device)                data['data'] = self.to_cpu(backbone.encode_image(image=inp, keep_dict=True))                cache[modality].add(data)            for bd in cache.values():                bd.save()        self.loaders = [DataLoader(bd, batch_size=self.batch_size,                                   num_workers=self.num_workers,                                   shuffle=self.shuffle) for k, bd in cache.items()]    def __iter__(self):        self.iter_loaders = [iter(loader) for loader in self.loaders]        return self    def __next__(self):        self.loader = self.get_loader()        try:            batch = next(self.loader)            if 'MM' not in self.modality:                # 单模态情况                batch['modality'] = [self.modality] * self.batch_size            elif batch['modality'][0] == 'MM':                batch['modality'] = [self.modality] * self.batch_size            return batch        except StopIteration:            self.remove_loader(self.loader)            return self.__next__()def get_loaders_from_args(args):    """    :param args: argparse object            : args.concept_bank: concept bank            : args.csv_path: csv path            : args.valid_only: only use valid data            : args.test_only: only use test data            : args.same_valid: use same valid data            : args.under_sample: under sample            : args.modality_shuffle: shuffle modality            : args.num_worker: num worker            : args.bz: batch size    :return:    """    train_trans, val_trans = get_trans_from_args(args)    return get_loaders(        train_trans=train_trans, val_trans=val_trans,        bz=args.bz, num_worker=args.num_worker, k=args.k, modality=args.modality,        imbalance=args.under_sample, out_channel=args.out_channel,        valid_only=args.valid_only, test_only=args.test_only,        same_valid=args.same_valid, modality_shuffle=args.modality_shuffle,        time_shuffle=args.time_shuffle, extra_data=args.extra_data, n_shot=args.n_shot    )def get_trans_from_args(args):    if hasattr(args, 'transform'):        train_trans = val_trans = args.transform    elif 'clip' in args.clip_name:        train_trans = args.concept_bank.transform        val_trans = args.concept_bank.transform    else:        train_trans = val_transforms(normalize=True) if args.no_data_aug else train_transforms(normalize=True)        val_trans = val_transforms(normalize=True)    return train_trans, val_transdef get_loaders(train_trans,                val_trans,                bz, k, modality,                num_worker=2, imbalance=False, out_channel=3,                valid_only=False, test_only=False,                same_valid=False, under_sample=False, modality_shuffle=False,                time_shuffle=False, extra_data=False, n_shot='full'):    reader = DataSplit(data_path=data_info['data_path'],                       csv_path=data_info['csv_path'],                       valid_only=valid_only,                       test_only=test_only,                       same_valid=same_valid,                       under_sample=under_sample,                       modality_shuffle=modality_shuffle,                       extra_data=extra_data,                       modality=modalities)    loader = [        MMDataLoader(reader=reader, mode='train', modality=modality, k_fold=k, batch_size=bz,                     imbalance=imbalance, num_worker=num_worker, noise=out_channel > 3,                     time_shuffle=time_shuffle, train_trans=train_trans,                     val_trans=val_trans, n_shot=n_shot),        MMDataLoader(reader=reader, mode='valid', modality=modality, k_fold=k, batch_size=bz,                     imbalance=False, num_worker=num_worker, time_shuffle=False,                     train_trans=train_trans, val_trans=val_trans, n_shot=n_shot),        MMDataLoader(reader=reader,                     mode='test',                     modality=modality,                     k_fold=k,                     batch_size=bz,                     imbalance=False,                     num_worker=num_worker,                     time_shuffle=False,                     train_trans=train_trans,                     val_trans=val_trans, n_shot=n_shot)    ]    return loader